---
layout: page
title: Software Usage Collection and Analysis with ELK at CAEN
---

<p>
CAEN supports 100s of software packages in its student computing labs
and in its HPC environment, and the list grows every year.  To help us
make decisions about software usage patterns, we wanted to collect and
analyze data on a per-title, per-computer, per-user basis.
</p>

<p>
By gathering software usage information for everything that CAEN
installs, we hope to:
</p>
<ul class="org-ul">
<li>get a better understanding of what software is used, so we can
remove titles that we donâ€™t need to be installing
</li>
<li>correlate titles with courses, and try to suggest that similar
courses use the same software to minimize what the students have to
learn, the instructors have to teach, and CAEN has to support
</li>
<li>make purchasing and labor decisions based on more than just an
educated guess
</li>
</ul>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">ELK</h2>
<div class="outline-text-2" id="text-1">
<p>
The open-source <a href="http://www.elasticsearch.org/overview/">ELK stack</a> allows us to process, aggregate, store,
search, and analyze logs with a lot of metadata from Windows and
Linux computers.
</p>
<ul class="org-ul">
<li>Elasticsearch is based on Apache Lucene and is a very fast,
distributed real-time search and analytics engine that offers a rich
query DSL via a REST API
</li>
<li>Logstash converts logs to JSON according to your rules and ships
them to Elasticsearch
</li>
<li>Kibana is a real-time visualization engine for Elastic- search
</li>
</ul>

<p>
A great place to start with ELK is
<a href="http://logstash.net/docs/1.4.2/tutorials/getting-started-with-logstash">http://logstash.net/docs/1.4.2/tutorials/getting-started-with-logstash</a>.
</p>

<p>
We also use the <a href="http://beaver.readthedocs.org/">Beaver Python daemon</a> to ship logs to Logstash from
the Linux Lab computers.
</p>

<p>
Elasticsearch also allows us to modify log entries after the fact,
so we are going to insert demographic and role data to augment the
username field. This data will include things like class standing
(Freshman, Sophomore, etc.), department, course enrollment, and
affiliation (Alumni, Student, Staff, etc.).
</p>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">Our ELK Environment</h2>
<div class="outline-text-2" id="text-2">
<p>
The ELK Environment at CAEN consists of a two-node Elasticsearch
cluster, a log aggregation server to collect the logs from Beaver,
and a Kibana web server to present some of the data.
</p>

<p>
The College of Engineering computer labs have about 1000 computers,
each of which dual-boots into either Linux or Windows.  When they
are running Linux, they use the Beaver Python daemon to deliver
their software usage information to an aggregation server that does
some processing and delivers the logs on to the Elasticsearch
cluster.  When the computers are running Windows, about a dozen of
them log their software usage.  This is a test before we deploy the
logging agents to the rest of the Windows computers.
</p>

<p>
The HPC Cluster comprises about 1500 computers, all of which run
Logstash as a daemon and deliver their logs via Redis to the
Elasticsearch cluster.
</p>

<p>
The configuration of each of these is different.  The Linux
computers in the computer labs use <a href="linux-lab-beaver.html">Beaver and its configuration</a> to
gather and ship software usage information from the <code>auditd</code> logs.
The Linux computers that make up the HPC cluster use <a href="linux-hpc-logstash.html">Logstash and
its configuration</a> to gather and ship software usage information from
their <code>auditd</code> logs.  The Windows computers in the computer labs use
the <i>Audit Process Creation on Success</i> and <i>Audit Process
Termination on Success</i> settings in the Group Policy to log those
events via the Windows Event log, where Logstash picks up those
entries using <a href="windows-lab-logstash.html">a Windows-specific Logstash configuration file</a>.
</p>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3">Results</h2>
<div class="outline-text-2" id="text-3">
<p>
Once the data is in Elasticsearch, it is replicated between our two
nodes and is available for searching.
</p>

<p>
Kibana is the web-based front-end that is part of the ELK
stack.<sup><a id="fnr.1" name="fnr.1" class="footref" href="#fn.1">1</a></sup> Some examples of representing software usage data and
other data in Kibana are <a href="kibana-examples.html">here</a>.
</p>

<p>
You can also query Elasticsearch directly, using either a URL query
string, or a JSON-structured <code>GET</code> query.  Version 3 of Kibana
doesn&rsquo;t support aggregating data (this is similar to SQL <code>JOIN</code>
statements), where as the JSON-structured queries do.  Additionally,
having data reported via a Python script can sometimes be more
useful than data reported via a web page.  Some examples of direct
queries are <a href="elasticsearch-queries.html">here</a>.
</p>
</div>
</div>

<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4">Other Uses of ELK at CAEN</h2>
<div class="outline-text-2" id="text-4">
</div><div id="outline-container-sec-4-1" class="outline-3">
<h3 id="sec-4-1">HPC Cluster</h3>
<div class="outline-text-3" id="text-4-1">
<p>
We are use ELK to aggregate logs from our high-performance computing
cluster.  <a href="http://www-personal.umich.edu/~msbritt/">Matt Britt</a> presented these results at MoabCon in 2014, the
recording of which is at
<a href="https://www.youtube.com/watch?v=JRvZk9Jk54M">https://www.youtube.com/watch?v=JRvZk9Jk54M</a>.
</p>
<iframe width="560" height="315" src="//www.youtube.com/embed/JRvZk9Jk54M" frameborder="0" allowfullscreen></iframe>
</div>
</div>
<div id="outline-container-sec-4-2" class="outline-3">
<h3 id="sec-4-2">Anomaly Detection</h3>
<div class="outline-text-3" id="text-4-2">
<p>
We are starting to use ELK for some simple anomaly detection.
After deploying software logging to the CAEN Lab computers running
Windows we noticed one computer launching AutoCad over and over
again with great frequency.
</p>


<div class="figure">
<p><img src="./images/elk-anomaly.png" alt="elk-anomaly.png" width="80%" />
</p>
<p><span class="figure-number">Figure 1:</span> An excessive number of launches of AutoCAD</p>
</div>

<p>
Upon further investigation the logout process for the last user of
that lab computer didn&rsquo;t complete and AutoCAD kept trying to
relaunch.
</p>

<p>
We haven&rsquo;t automated any anomoly detection, but are looking into
it.  The O&rsquo;Reilly article <a href="http://radar.oreilly.com/2013/08/anomalies-and-patterns-in-machine-data.html">Surfacing anomalies and patterns in
Machine Data</a> is one example of what is available.
</p>
</div>
</div>

<div id="outline-container-sec-4-3" class="outline-3">
<h3 id="sec-4-3">Geographic Data</h3>
<div class="outline-text-3" id="text-4-3">
<p>
We use ELK to store data on our <a href="https://www.globus.org/">Globus</a>/GridFTP transfers, and we
can see places where people are transferring data.
</p>

<div class="figure">
<p><img src="./images/kibana-globus-map.png" alt="kibana-globus-map.png" width="80%" />
</p>
<p><span class="figure-number">Figure 2:</span> Where data is being transferred</p>
</div>

<p>
ELK can also show us where the last 1000 transfers started or
ended.  Most of them are on campus, which is interesting and bears
further consideration (would a shared filesystem be better?  why
Globus over <code>sftp</code>?  why not leave the data on the cluster?).
</p>

<div class="figure">
<p><img src="./images/kibana-globus-map-last1000.png" alt="kibana-globus-map-last1000.png" width="80%" />
</p>
<p><span class="figure-number">Figure 3:</span> The last 1000 Globus transfers</p>
</div>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" name="fn.1" class="footnum" href="#fnr.1">1</a></sup> <p class="footpara">
Other analysis and presentation engines can also be used with
Logstash data.  We also use Graphite to ingest data from Logstash from
our Luster Object Storage Servers, skipping Elasticsearch altogether.
</p></div>


</div>
</div>
